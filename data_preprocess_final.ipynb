{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge original data with new sentiment and topic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZIQING002\\AppData\\Local\\Temp\\ipykernel_31388\\3179780348.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"./data/eastmoney_bert.csv\", usecols=['item_title', 'item_author_cate', 'article_author', 'article_source_cate',\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/eastmoney_bert.csv\", usecols=['item_title', 'item_author_cate', 'article_author', 'article_source_cate',\n",
    "       'year', 'month',\n",
    "       'eastmoney_robo_journalism', 'media_robo_journalism', 'SMA_robo_journalism', \n",
    "       'stock_code',\n",
    "       'viral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentiment_label'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data = pd.read_csv(\"./lda/sentiment_score.csv\", usecols=['sentiment_label'])\n",
    "sentiment_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'] = sentiment_data['sentiment_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_data = pd.read_csv(\"./lda/eastmoney_topic.csv\",usecols=['dominant_topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topic'] = topic_data['dominant_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_code</th>\n",
       "      <th>IndustryCode1</th>\n",
       "      <th>IndustryCode2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>J</td>\n",
       "      <td>J66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>K</td>\n",
       "      <td>K70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>C</td>\n",
       "      <td>C27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004</td>\n",
       "      <td>I</td>\n",
       "      <td>I65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005</td>\n",
       "      <td>N</td>\n",
       "      <td>N77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5405</th>\n",
       "      <td>600270</td>\n",
       "      <td>G</td>\n",
       "      <td>G58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406</th>\n",
       "      <td>600401</td>\n",
       "      <td>C</td>\n",
       "      <td>C39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>600432</td>\n",
       "      <td>C</td>\n",
       "      <td>C32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5408</th>\n",
       "      <td>600747</td>\n",
       "      <td>F</td>\n",
       "      <td>F51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5409</th>\n",
       "      <td>600806</td>\n",
       "      <td>C</td>\n",
       "      <td>C34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5410 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     stock_code IndustryCode1 IndustryCode2\n",
       "0        000001             J           J66\n",
       "1        000002             K           K70\n",
       "2        000004             C           C27\n",
       "3        000004             I           I65\n",
       "4        000005             N           N77\n",
       "...         ...           ...           ...\n",
       "5405     600270             G           G58\n",
       "5406     600401             C           C39\n",
       "5407     600432             C           C32\n",
       "5408     600747             F           F51\n",
       "5409     600806             C           C34\n",
       "\n",
       "[5410 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyreadr\n",
    "\n",
    "# Load the RData file\n",
    "result = pyreadr.read_r('./data/industry_csrc2012_matched.RData')\n",
    "ind_data = result['industry_csrc2012_matched']\n",
    "ind_data[['stock_code','IndustryCode1','IndustryCode2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_data[['stock_code','IndustryCode1','IndustryCode2']]\n",
    "ind_data['stock_code'] = ind_data['stock_code'].astype(int)\n",
    "ind_data = ind_data.drop_duplicates(subset=['stock_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6603696"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = data.merge(ind_data[['stock_code','IndustryCode1','IndustryCode2']], on='stock_code', how='left')\n",
    "len(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(merged_data['IndustryCode1'].isna().sum())\n",
    "merged_data['IndustryCode1'].fillna('others', inplace=True)\n",
    "print(merged_data['IndustryCode1'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13250\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZIQING002\\AppData\\Local\\Temp\\ipykernel_31388\\4104984895.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_data['IndustryCode2'].fillna('others', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "print(merged_data['IndustryCode2'].isna().sum())\n",
    "merged_data['IndustryCode2'].fillna('others', inplace=True)\n",
    "print(merged_data['IndustryCode2'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_title', 'article_author', 'year', 'month',\n",
       "       'eastmoney_robo_journalism', 'media_robo_journalism',\n",
       "       'SMA_robo_journalism', 'viral', 'article_source_cate',\n",
       "       'item_author_cate', 'stock_code', 'sentiment', 'topic', 'IndustryCode1',\n",
       "       'IndustryCode2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = merged_data\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = data['year'].astype(int)\n",
    "data['month'] = data['month'].astype(int)\n",
    "data['eastmoney_robo_journalism'] = data['eastmoney_robo_journalism'].astype(int)\n",
    "data['media_robo_journalism'] = data['media_robo_journalism'].astype(int)\n",
    "data['SMA_robo_journalism'] = data['SMA_robo_journalism'].astype(int)\n",
    "data['stock_code'] = data['stock_code'].astype(int)\n",
    "data['topic'] = data['topic'].astype(int)\n",
    "data['sentiment'] = data['sentiment'].map({'Neutral': 1, 'Positive': 2, 'Negative': 0}).astype(int)\n",
    "data['viral'] = data['viral'].astype(int)\n",
    "data['item_author'] = data['item_author_cate'].astype(str)\n",
    "data['article_author'] = data['article_author'].astype(str)\n",
    "data['article_source'] = data['article_source_cate'].astype(str)\n",
    "data.loc[data['article_source'].str.len() > 40, 'article_source'] = \"东方财富Choice数据\"\n",
    "data.loc[data['article_source']=='东方财富Choice数据■', 'article_source'] = \"东方财富Choice数据\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_title</th>\n",
       "      <th>article_author</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>eastmoney_robo_journalism</th>\n",
       "      <th>media_robo_journalism</th>\n",
       "      <th>SMA_robo_journalism</th>\n",
       "      <th>viral</th>\n",
       "      <th>article_source_cate</th>\n",
       "      <th>item_author_cate</th>\n",
       "      <th>stock_code</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic</th>\n",
       "      <th>IndustryCode1</th>\n",
       "      <th>IndustryCode2</th>\n",
       "      <th>item_author</th>\n",
       "      <th>article_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6603696</td>\n",
       "      <td>6603696</td>\n",
       "      <td>6.603696e+06</td>\n",
       "      <td>6.603696e+06</td>\n",
       "      <td>6.603696e+06</td>\n",
       "      <td>6.603696e+06</td>\n",
       "      <td>6.603696e+06</td>\n",
       "      <td>6.603696e+06</td>\n",
       "      <td>6603696</td>\n",
       "      <td>6603696</td>\n",
       "      <td>6.603696e+06</td>\n",
       "      <td>6.603696e+06</td>\n",
       "      <td>6.603696e+06</td>\n",
       "      <td>6603696</td>\n",
       "      <td>6603696</td>\n",
       "      <td>6603696</td>\n",
       "      <td>6603696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6405069</td>\n",
       "      <td>5799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>5158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>82</td>\n",
       "      <td>5158</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>百联股份今日热门盘点</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>东方财富</td>\n",
       "      <td>股市胖虎</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C39</td>\n",
       "      <td>股市胖虎</td>\n",
       "      <td>东方财富</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>116</td>\n",
       "      <td>3253327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4181680</td>\n",
       "      <td>426876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4241203</td>\n",
       "      <td>765036</td>\n",
       "      <td>426876</td>\n",
       "      <td>4181680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.021472e+03</td>\n",
       "      <td>6.593989e+00</td>\n",
       "      <td>5.504324e-01</td>\n",
       "      <td>1.353547e-01</td>\n",
       "      <td>2.612287e-01</td>\n",
       "      <td>4.778476e-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.414867e+05</td>\n",
       "      <td>1.081507e+00</td>\n",
       "      <td>2.177143e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.164505e+00</td>\n",
       "      <td>3.552674e+00</td>\n",
       "      <td>4.974501e-01</td>\n",
       "      <td>3.421020e-01</td>\n",
       "      <td>4.393043e-01</td>\n",
       "      <td>2.133105e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.724346e+05</td>\n",
       "      <td>4.752776e-01</td>\n",
       "      <td>1.072451e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.017000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.021000e+03</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.522000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.022000e+03</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.008900e+05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.022000e+03</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.010000e+05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.023000e+03</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.889810e+05</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_title article_author          year         month  \\\n",
       "count      6603696        6603696  6.603696e+06  6.603696e+06   \n",
       "unique     6405069           5799           NaN           NaN   \n",
       "top     百联股份今日热门盘点              0           NaN           NaN   \n",
       "freq           116        3253327           NaN           NaN   \n",
       "mean           NaN            NaN  2.021472e+03  6.593989e+00   \n",
       "std            NaN            NaN  1.164505e+00  3.552674e+00   \n",
       "min            NaN            NaN  2.017000e+03  1.000000e+00   \n",
       "25%            NaN            NaN  2.021000e+03  4.000000e+00   \n",
       "50%            NaN            NaN  2.022000e+03  7.000000e+00   \n",
       "75%            NaN            NaN  2.022000e+03  1.000000e+01   \n",
       "max            NaN            NaN  2.023000e+03  1.200000e+01   \n",
       "\n",
       "        eastmoney_robo_journalism  media_robo_journalism  SMA_robo_journalism  \\\n",
       "count                6.603696e+06           6.603696e+06         6.603696e+06   \n",
       "unique                        NaN                    NaN                  NaN   \n",
       "top                           NaN                    NaN                  NaN   \n",
       "freq                          NaN                    NaN                  NaN   \n",
       "mean                 5.504324e-01           1.353547e-01         2.612287e-01   \n",
       "std                  4.974501e-01           3.421020e-01         4.393043e-01   \n",
       "min                  0.000000e+00           0.000000e+00         0.000000e+00   \n",
       "25%                  0.000000e+00           0.000000e+00         0.000000e+00   \n",
       "50%                  1.000000e+00           0.000000e+00         0.000000e+00   \n",
       "75%                  1.000000e+00           0.000000e+00         1.000000e+00   \n",
       "max                  1.000000e+00           1.000000e+00         1.000000e+00   \n",
       "\n",
       "               viral article_source_cate item_author_cate    stock_code  \\\n",
       "count   6.603696e+06             6603696          6603696  6.603696e+06   \n",
       "unique           NaN                 219             5158           NaN   \n",
       "top              NaN                东方财富             股市胖虎           NaN   \n",
       "freq             NaN             4181680           426876           NaN   \n",
       "mean    4.778476e-02                 NaN              NaN  3.414867e+05   \n",
       "std     2.133105e-01                 NaN              NaN  2.724346e+05   \n",
       "min     0.000000e+00                 NaN              NaN  1.000000e+00   \n",
       "25%     0.000000e+00                 NaN              NaN  2.522000e+03   \n",
       "50%     0.000000e+00                 NaN              NaN  3.008900e+05   \n",
       "75%     0.000000e+00                 NaN              NaN  6.010000e+05   \n",
       "max     1.000000e+00                 NaN              NaN  6.889810e+05   \n",
       "\n",
       "           sentiment         topic IndustryCode1 IndustryCode2 item_author  \\\n",
       "count   6.603696e+06  6.603696e+06       6603696       6603696     6603696   \n",
       "unique           NaN           NaN            20            82        5158   \n",
       "top              NaN           NaN             C           C39        股市胖虎   \n",
       "freq             NaN           NaN       4241203        765036      426876   \n",
       "mean    1.081507e+00  2.177143e+00           NaN           NaN         NaN   \n",
       "std     4.752776e-01  1.072451e+00           NaN           NaN         NaN   \n",
       "min     0.000000e+00  0.000000e+00           NaN           NaN         NaN   \n",
       "25%     1.000000e+00  2.000000e+00           NaN           NaN         NaN   \n",
       "50%     1.000000e+00  2.000000e+00           NaN           NaN         NaN   \n",
       "75%     1.000000e+00  3.000000e+00           NaN           NaN         NaN   \n",
       "max     2.000000e+00  4.000000e+00           NaN           NaN         NaN   \n",
       "\n",
       "       article_source  \n",
       "count         6603696  \n",
       "unique            219  \n",
       "top              东方财富  \n",
       "freq          4181680  \n",
       "mean              NaN  \n",
       "std               NaN  \n",
       "min               NaN  \n",
       "25%               NaN  \n",
       "50%               NaN  \n",
       "75%               NaN  \n",
       "max               NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['item_author_index'], uniques = pd.factorize(data['item_author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['article_author_index'], uniques = pd.factorize(data['article_author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['article_source_index'], uniques = pd.factorize(data['article_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stock_code_index'], uniques = pd.factorize(data['stock_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ind_code1_index'], uniques = pd.factorize(data['IndustryCode1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ind_code2_index'], uniques = pd.factorize(data['IndustryCode2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./data/eastmoney_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank author info based on past month's stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZIQING002\\AppData\\Local\\Temp\\ipykernel_31388\\2751206451.py:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('./data/eastmoney_full.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./data/eastmoney_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_title', 'article_author', 'year', 'month',\n",
       "       'eastmoney_robo_journalism', 'media_robo_journalism',\n",
       "       'SMA_robo_journalism', 'viral', 'article_source_cate',\n",
       "       'item_author_cate', 'stock_code', 'sentiment', 'topic', 'IndustryCode1',\n",
       "       'IndustryCode2', 'item_author', 'article_source', 'item_author_index',\n",
       "       'article_author_index', 'article_source_index', 'stock_code_index',\n",
       "       'ind_code1_index', 'ind_code2_index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'year_month' column for easier handling\n",
    "data['year_month'] = pd.to_datetime(data['year'].astype(str) + '-' + data['month'].astype(str) + '-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_by_prev_month(data, col):\n",
    "    # Group by 'year', 'month', and col to be ranked to count the number of posts by each author in each month\n",
    "    monthly_count = data.groupby(['year', 'month', col]).size().reset_index(name='post_count')\n",
    "\n",
    "    # Create a shifted version of 'author_monthly_posts' to simulate the \"previous month\"\n",
    "    monthly_count['year_month'] = pd.to_datetime(monthly_count['year'].astype(str) + '-' + monthly_count['month'].astype(str) + '-01')\n",
    "    monthly_count['previous_year_month'] = monthly_count['year_month'] - pd.DateOffset(months=1)\n",
    "\n",
    "    # Split 'previous_year_month' into year and month to prepare for the merge\n",
    "    monthly_count['previous_year'] = monthly_count['previous_year_month'].dt.year\n",
    "    monthly_count['previous_month'] = monthly_count['previous_year_month'].dt.month\n",
    "\n",
    "    # Split 'previous_year_month' into year and month to prepare for the merge\n",
    "    monthly_count['previous_year'] = monthly_count['previous_year_month'].dt.year\n",
    "    monthly_count['previous_month'] = monthly_count['previous_year_month'].dt.month\n",
    "\n",
    "    # Rank authors by their post count within the previous month\n",
    "    monthly_count[f\"{col}_rank\"] = monthly_count.groupby(['previous_year', 'previous_month'])['post_count'].rank(ascending=False, method='dense')\n",
    "\n",
    "    # Merge the rank information back into the original data by matching on 'year', 'month', and 'item_author_index'\n",
    "    data_with_rank = pd.merge(data, \n",
    "                            monthly_count[[col, 'year', 'month', f\"{col}_rank\"]], \n",
    "                            how='left', \n",
    "                            left_on=[col, 'year', 'month'], \n",
    "                            right_on=[col, 'year', 'month'])\n",
    "\n",
    "    # Display the updated data with the author rank in the previous month\n",
    "    print(data_with_rank[['year', 'month', col, f\"{col}_rank\"]])\n",
    "\n",
    "    return data_with_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim_by_rank(df, col, max_rank=10):\n",
    "    # Create a mask for ranks that are greater than max_rank\n",
    "    rank_mask = df[f\"{col}_rank\"] > max_rank\n",
    "\n",
    "    # Set ranks exceeding max_rank to max_rank + 1\n",
    "    df.loc[rank_mask, f\"{col}_rank\"] = max_rank + 1\n",
    "    \n",
    "    # Set corresponding column values (e.g., 'item' or 'author') to max_index + 1\n",
    "    # df.loc[rank_mask, col] = max_index + 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         year  month  item_author_index  item_author_index_rank\n",
      "0        2020      5                  0                     3.0\n",
      "1        2020      8                  1                     2.0\n",
      "2        2020      9                  2                     1.0\n",
      "3        2020      9                  2                     1.0\n",
      "4        2020      9                  2                     1.0\n",
      "...       ...    ...                ...                     ...\n",
      "6603691  2023      8               5111                    55.0\n",
      "6603692  2023      8               5111                    55.0\n",
      "6603693  2023      8               5111                    55.0\n",
      "6603694  2023      8               5111                    55.0\n",
      "6603695  2023      8               5111                    55.0\n",
      "\n",
      "[6603696 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data = rank_by_prev_month(data, 'item_author_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         year  month  item_author_index  item_author_index_rank\n",
      "0        2020      5                  0                     3.0\n",
      "1        2020      8                  1                     2.0\n",
      "2        2020      9                  2                     1.0\n",
      "3        2020      9                  2                     1.0\n",
      "4        2020      9                  2                     1.0\n",
      "...       ...    ...                ...                     ...\n",
      "6603691  2023      8               5111                    11.0\n",
      "6603692  2023      8               5111                    11.0\n",
      "6603693  2023      8               5111                    11.0\n",
      "6603694  2023      8               5111                    11.0\n",
      "6603695  2023      8               5111                    11.0\n",
      "\n",
      "[6603696 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# data = data.apply(reduce_dim_by_rank, axis=1, col='item_author_index', max_index=data['item_author_index'].max())\n",
    "data = reduce_dim_by_rank(data, 'item_author_index', max_rank=10)\n",
    "print(data[['year', 'month', 'item_author_index', \"item_author_index_rank\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         year  month  article_author_index  article_author_index_rank\n",
      "0        2020      5                     0                        1.0\n",
      "1        2020      8                     0                        1.0\n",
      "2        2020      9                     0                        1.0\n",
      "3        2020      9                     0                        1.0\n",
      "4        2020      9                     0                        1.0\n",
      "...       ...    ...                   ...                        ...\n",
      "6603691  2023      8                     0                        2.0\n",
      "6603692  2023      8                     0                        2.0\n",
      "6603693  2023      8                   452                       48.0\n",
      "6603694  2023      8                     0                        2.0\n",
      "6603695  2023      8                     0                        2.0\n",
      "\n",
      "[6603696 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data = rank_by_prev_month(data, 'article_author_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         year  month  article_author_index  article_author_index_rank\n",
      "0        2020      5                     0                        1.0\n",
      "1        2020      8                     0                        1.0\n",
      "2        2020      9                     0                        1.0\n",
      "3        2020      9                     0                        1.0\n",
      "4        2020      9                     0                        1.0\n",
      "...       ...    ...                   ...                        ...\n",
      "6603691  2023      8                     0                        2.0\n",
      "6603692  2023      8                     0                        2.0\n",
      "6603693  2023      8                   452                       11.0\n",
      "6603694  2023      8                     0                        2.0\n",
      "6603695  2023      8                     0                        2.0\n",
      "\n",
      "[6603696 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data = reduce_dim_by_rank(data, 'article_author_index', max_rank=10)\n",
    "print(data[['year', 'month', 'article_author_index', \"article_author_index_rank\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         year  month  article_source_index  article_source_index_rank\n",
      "0        2020      5                     0                        1.0\n",
      "1        2020      8                     0                        1.0\n",
      "2        2020      9                     1                        1.0\n",
      "3        2020      9                     1                        1.0\n",
      "4        2020      9                     1                        1.0\n",
      "...       ...    ...                   ...                        ...\n",
      "6603691  2023      8                    30                       14.0\n",
      "6603692  2023      8                    57                       20.0\n",
      "6603693  2023      8                     6                       15.0\n",
      "6603694  2023      8                    21                       21.0\n",
      "6603695  2023      8                    15                        8.0\n",
      "\n",
      "[6603696 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data = rank_by_prev_month(data, 'article_source_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         year  month  article_source_index  article_source_index_rank\n",
      "0        2020      5                     0                        1.0\n",
      "1        2020      8                     0                        1.0\n",
      "2        2020      9                     1                        1.0\n",
      "3        2020      9                     1                        1.0\n",
      "4        2020      9                     1                        1.0\n",
      "...       ...    ...                   ...                        ...\n",
      "6603691  2023      8                    30                       11.0\n",
      "6603692  2023      8                    57                       11.0\n",
      "6603693  2023      8                     6                       11.0\n",
      "6603694  2023      8                    21                       11.0\n",
      "6603695  2023      8                    15                        8.0\n",
      "\n",
      "[6603696 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data = reduce_dim_by_rank(data, 'article_source_index', max_rank=10)\n",
    "print(data[['year', 'month', 'article_source_index', \"article_source_index_rank\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['item_title', 'item_author', 'article_author',\n",
    "       'article_source', 'year', 'month', 'eastmoney_robo_journalism',\n",
    "       'media_robo_journalism', 'SMA_robo_journalism', 'stock_code', 'viral',\n",
    "       'sentiment', 'topic', 'item_author_index', 'article_author_index',\n",
    "       'article_source_index', 'stock_code_index', 'year_month', 'ind_code1_index', 'ind_code2_index',\n",
    "       'item_author_index_rank', 'article_author_index_rank', 'article_source_index_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./data/eastmoney_full_ranked.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save metadata: unique value count for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6603696"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ft = ['month', \n",
    "            'ind_code1_index',\n",
    "            'ind_code2_index',\n",
    "            'sentiment',\n",
    "            'topic']\n",
    "author_ft = ['eastmoney_robo_journalism',\n",
    "                'media_robo_journalism',\n",
    "                'SMA_robo_journalism',\n",
    "                'item_author_index_rank',\n",
    "                'article_author_index_rank',\n",
    "                'article_source_index_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ft_unique_count = [int(data[x].max()+1) for x in post_ft]\n",
    "author_ft_unique_count = [int(data[x].max()+1) for x in author_ft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([13, 20, 82, 3, 5], [2, 2, 2, 12, 12, 12])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_ft_unique_count, author_ft_unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "meta_data = (post_ft_unique_count, author_ft_unique_count)\n",
    "with open('./data/bpr_v3_meta.pkl','wb') as f:\n",
    "    pickle.dump(meta_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample negative BPR samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data choronologically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZIQING002\\AppData\\Local\\Temp\\ipykernel_31388\\195616194.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('./data/eastmoney_full_ranked.csv', usecols=['item_title', 'item_author', 'article_author',\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./data/eastmoney_full_ranked.csv', usecols=['item_title', 'item_author', 'article_author',\n",
    "       'article_source', 'year', 'month', 'eastmoney_robo_journalism',\n",
    "       'media_robo_journalism', 'SMA_robo_journalism', 'stock_code', 'viral',\n",
    "       'sentiment', 'topic', 'item_author_index', 'article_author_index',\n",
    "       'article_source_index', 'stock_code_index', 'year_month', 'ind_code1_index', 'ind_code2_index',\n",
    "       'item_author_index_rank', 'article_author_index_rank', 'article_source_index_rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First round:\n",
    "- Train 2017-01-01, 2021.12.13 --> train_bpr1.csv\n",
    "- Valid 2022-01-01, 2022-06-30 --> valid_bpr.csv\n",
    "- Test  2022-07-01, 2022-09-30 --> test1.csv\n",
    "\n",
    "Second round: \n",
    "- Train 2022-07-01, 2022-09-30 --> train_bpr2.csv (test1)\n",
    "- Test  2022-10-01, 2022-12-31 --> test2.csv\n",
    "\n",
    "Third round:\n",
    "- Train 2022-10-01, 2022-12-31 --> train_bpr3.csv (test2)\n",
    "- Test  2023-01-01, 2022-03-31 --> test3.csv\n",
    "\n",
    "Fourth round:\n",
    "- Train 2023-01-01, 2022-03-31 --> train_bpr4.csv\n",
    "- Test  2023-04-01, 2023-08-31 --> test4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first round \n",
    "\n",
    "# Define the date ranges for train, valid, and test sets\n",
    "train1_start = '2017-01-01'\n",
    "train1_end = '2021-12-31'\n",
    "valid1_start = '2022-01-01'\n",
    "valid1_end = '2022-06-30'\n",
    "test1_start = '2022-07-01'\n",
    "test1_end = '2022-09-30'\n",
    "\n",
    "test2_start = '2022-10-01'\n",
    "test2_end = '2022-12-31'\n",
    "\n",
    "test3_start = '2023-01-01'\n",
    "test3_end = '2023-03-31'\n",
    "\n",
    "test4_start = '2023-04-01'\n",
    "test4_end = '2023-08-31'\n",
    "\n",
    "# Split the data based on the defined ranges\n",
    "train_set = data[(data['year_month'] >= train1_start) & (data['year_month'] <= train1_end)]\n",
    "valid_set = data[(data['year_month'] >= valid1_start) & (data['year_month'] <= valid1_end)]\n",
    "test1_set = data[(data['year_month'] >= test1_start) & (data['year_month'] <= test1_end)]\n",
    "test2_set = data[(data['year_month'] >= test2_start) & (data['year_month'] <= test2_end)]\n",
    "test3_set = data[(data['year_month'] >= test3_start) & (data['year_month'] <= test3_end)]\n",
    "test4_set = data[(data['year_month'] >= test4_start) & (data['year_month'] <= test4_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3208461, 1112397, 451944, 423289, 426113, 981492)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(valid_set), len(test1_set), len(test2_set), len(test3_set), len(test4_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(259955, 9436, 4506, 9963, 11906, 19790)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set[train_set['viral']==1]), len(valid_set[valid_set['viral']==1]), len(test1_set[test1_set['viral']==1]), len(test2_set[test2_set['viral']==1]), len(test3_set[test3_set['viral']==1]), len(test4_set[test4_set['viral']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(\"./data/train1.csv\", index=False)\n",
    "valid_set.to_csv(\"./data/valid1.csv\", index=False)\n",
    "test1_set.to_csv(\"./data/test1.csv\", index=False)\n",
    "test2_set.to_csv(\"./data/test2.csv\", index=False)\n",
    "test3_set.to_csv(\"./data/test3.csv\", index=False)\n",
    "test4_set.to_csv(\"./data/test4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample bpr data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZIQING002\\AppData\\Local\\Temp\\ipykernel_31388\\2589231190.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_set = pd.read_csv(\"./data/train1.csv\")\n",
      "C:\\Users\\ZIQING002\\AppData\\Local\\Temp\\ipykernel_31388\\2589231190.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  valid_set = pd.read_csv(\"./data/valid1.csv\")\n",
      "C:\\Users\\ZIQING002\\AppData\\Local\\Temp\\ipykernel_31388\\2589231190.py:3: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test1_set = pd.read_csv(\"./data/test1.csv\")\n",
      "C:\\Users\\ZIQING002\\AppData\\Local\\Temp\\ipykernel_31388\\2589231190.py:4: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test2_set = pd.read_csv(\"./data/test2.csv\")\n",
      "C:\\Users\\ZIQING002\\AppData\\Local\\Temp\\ipykernel_31388\\2589231190.py:5: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test3_set = pd.read_csv(\"./data/test3.csv\")\n",
      "C:\\Users\\ZIQING002\\AppData\\Local\\Temp\\ipykernel_31388\\2589231190.py:6: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test4_set = pd.read_csv(\"./data/test4.csv\")\n"
     ]
    }
   ],
   "source": [
    "train_set = pd.read_csv(\"./data/train1.csv\")\n",
    "valid_set = pd.read_csv(\"./data/valid1.csv\")\n",
    "test1_set = pd.read_csv(\"./data/test1.csv\")\n",
    "test2_set = pd.read_csv(\"./data/test2.csv\")\n",
    "test3_set = pd.read_csv(\"./data/test3.csv\")\n",
    "test4_set = pd.read_csv(\"./data/test4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def sample_bpr(data, aux_data=[]):\n",
    "\n",
    "    #combine data\n",
    "    ref_data = pd.concat([data] + aux_data, axis=0, ignore_index=True)\n",
    "\n",
    "    # Separate positive and negative samples\n",
    "    positive_samples = data[data['viral'] == 1]\n",
    "    negative_samples = ref_data[ref_data['viral'] == 0]\n",
    "\n",
    "    # Create dictionaries to group negative samples by criteria for faster lookup\n",
    "    negatives_by_criteria1 = negative_samples.groupby(['item_author', 'article_author', 'article_source'])\n",
    "    negatives_by_criteria21 = negative_samples.groupby('article_source')\n",
    "    negatives_by_criteria22 = negative_samples.groupby('stock_code')\n",
    "\n",
    "    # List to hold the result rows\n",
    "    sampled_negatives = []\n",
    "\n",
    "    # Loop over each positive sample\n",
    "    for _, positive_row in tqdm(positive_samples.iterrows(), total=len(positive_samples)):\n",
    "        # Try to get negative samples by Criteria 1\n",
    "        key1 = (positive_row['item_author'], positive_row['article_author'], positive_row['article_source'])\n",
    "        matching_negatives_criteria1 = negatives_by_criteria1.get_group(key1) if key1 in negatives_by_criteria1.groups else pd.DataFrame()\n",
    "\n",
    "        # If Criteria 1 matches fewer than 3 rows, add Criteria 2 matches\n",
    "        if len(matching_negatives_criteria1) < 3:\n",
    "            key2 = positive_row['article_source']\n",
    "            matching_negatives_criteria21 = negatives_by_criteria21.get_group(key2) if key2 in negatives_by_criteria21.groups else pd.DataFrame()\n",
    "            matching_negatives_criteria21 = matching_negatives_criteria21.sample(n=3, replace=False) if len(matching_negatives_criteria21) >= 3 else matching_negatives_criteria21\n",
    "            key3 = positive_row['stock_code']\n",
    "            matching_negatives_criteria22 = negatives_by_criteria22.get_group(key3) if key3 in negatives_by_criteria22.groups else pd.DataFrame()\n",
    "            matching_negatives_criteria22 = matching_negatives_criteria22.sample(n=3, replace=False) if len(matching_negatives_criteria22) >= 3 else matching_negatives_criteria22\n",
    "            combined_matches = pd.concat([matching_negatives_criteria1, matching_negatives_criteria21, matching_negatives_criteria22]).drop_duplicates()\n",
    "            sampled = combined_matches.sample(n=3, replace=False) if len(combined_matches) >= 3 else combined_matches\n",
    "        else:\n",
    "            # Criteria 1 provides enough matches\n",
    "            sampled = matching_negatives_criteria1.sample(n=3, replace=False)\n",
    "        \n",
    "        if len(sampled)<3:\n",
    "            print(key1, key2, key3)\n",
    "            print(negatives_by_criteria22.groups.keys())\n",
    "            return key3, negatives_by_criteria22\n",
    "        \n",
    "        # Prefix columns and append each pair to result list\n",
    "        for _, negative_row in sampled.iterrows():\n",
    "            combined_row = pd.concat([positive_row, negative_row.add_prefix('neg_')])\n",
    "            sampled_negatives.append(combined_row)\n",
    "\n",
    "    # Concatenate all combined rows into a final DataFrame\n",
    "    final_sampled_negatives = pd.DataFrame(sampled_negatives).reset_index(drop=True)\n",
    "\n",
    "    return final_sampled_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259955/259955 [23:37<00:00, 183.43it/s] \n"
     ]
    }
   ],
   "source": [
    "# round1\n",
    "train_bpr1 = sample_bpr(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3208461 259955 779865\n"
     ]
    }
   ],
   "source": [
    "train_bpr1.to_csv('./data/train_bpr1.csv', index=False)\n",
    "print(len(train_set), len(train_set[train_set['viral']==1]), len(train_bpr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9436/9436 [00:37<00:00, 249.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112397 9436 28308\n"
     ]
    }
   ],
   "source": [
    "valid_bpr = sample_bpr(valid_set, [train_set])\n",
    "valid_bpr.to_csv('./data/valid_bpr.csv', index=False)\n",
    "print(len(valid_set), len(valid_set[valid_set['viral']==1]), len(valid_bpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4506/4506 [00:29<00:00, 153.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451944 4506 13518\n"
     ]
    }
   ],
   "source": [
    "# round2\n",
    "train_bpr2 = sample_bpr(test1_set, [train_set, valid_set])\n",
    "train_bpr2.to_csv('./data/train_bpr2.csv', index=False)\n",
    "print(len(test1_set), len(test1_set[test1_set['viral']==1]), len(train_bpr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9963/9963 [00:50<00:00, 198.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423289 9963 29889\n"
     ]
    }
   ],
   "source": [
    "# round3\n",
    "train_bpr3 = sample_bpr(test2_set, [train_set, valid_set, test1_set])\n",
    "train_bpr3.to_csv('./data/train_bpr3.csv', index=False)\n",
    "print(len(test2_set), len(test2_set[test2_set['viral']==1]), len(train_bpr3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11906/11906 [01:19<00:00, 149.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426113 11906 35718\n"
     ]
    }
   ],
   "source": [
    "# round4\n",
    "train_bpr4 = sample_bpr(test3_set, [train_set, valid_set, test1_set])\n",
    "train_bpr4.to_csv('./data/train_bpr4.csv', index=False)\n",
    "print(len(test3_set), len(test3_set[test3_set['viral']==1]), len(train_bpr4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appendix: bpr sample analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZIQING002\\AppData\\Local\\Temp\\ipykernel_26456\\2214158559.py:3: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data = pd.read_csv('./data/eastmoney_bert.csv', usecols=['viral','item_author_cate', 'article_author', 'article_source_cate', 'stock_code', 'year', 'month'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# check avg matching statistics\n",
    "raw_data = pd.read_csv('./data/eastmoney_bert.csv', usecols=['viral','item_author_cate', 'article_author', 'article_source_cate', 'stock_code', 'year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['item_author'] = raw_data['item_author_cate'].astype(str)\n",
    "raw_data['article_author'] = raw_data['article_author'].astype(str)\n",
    "raw_data['article_source'] = raw_data['article_source_cate'].astype(str)\n",
    "raw_data.loc[raw_data['article_source'].str.len() > 40, 'article_source'] = \"东方财富Choice数据\"\n",
    "raw_data.loc[raw_data['article_source']=='东方财富Choice数据■', 'article_source'] = \"东方财富Choice数据\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3208461\n"
     ]
    }
   ],
   "source": [
    "raw_data['year_month'] = pd.to_datetime(raw_data['year'].astype(int).astype(str) + '-' + raw_data['month'].astype(int).astype(str) + '-01')\n",
    "raw_train = raw_data[(raw_data['year_month'] >= '2017-01-01') & (raw_data['year_month'] <= '2021-12-31')]\n",
    "print(len(raw_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  article_author    year  month  viral article_source_cate item_author_cate  \\\n",
      "0              0  2021.0    5.0    1.0                东方财富            胖猫周周见   \n",
      "1              0  2021.0    5.0    1.0                东方财富            大眼看两融   \n",
      "2              0  2021.0    5.0    1.0                东方财富            胖猫周周见   \n",
      "3              0  2021.0    5.0    1.0                东方财富            大眼看两融   \n",
      "4              0  2021.0    5.0    1.0            Choice数据     股友8635377Z3T   \n",
      "\n",
      "   stock_code   item_author article_source year_month  neg_count1  \n",
      "0           1         胖猫周周见           东方财富 2021-05-01     44390.0  \n",
      "1           1         大眼看两融           东方财富 2021-05-01     72999.0  \n",
      "2           1         胖猫周周见           东方财富 2021-05-01     44390.0  \n",
      "3           1         大眼看两融           东方财富 2021-05-01     72999.0  \n",
      "4           1  股友8635377Z3T       Choice数据 2021-05-01       311.0  \n"
     ]
    }
   ],
   "source": [
    "positive_rows = raw_train[raw_train['viral'] == 1]\n",
    "negative_rows = raw_train[raw_train['viral'] == 0]\n",
    "\n",
    "\n",
    "# C1\n",
    "neg_count_df = negative_rows.groupby(['item_author', 'article_author', 'article_source']).size().reset_index(name='neg_count1')\n",
    "# Merge with unique combinations to get primary counts\n",
    "merged = positive_rows.merge(neg_count_df, on=['item_author', 'article_author', 'article_source'], how='left',suffixes=('', '1')).fillna(0)\n",
    "print(merged.head())\n",
    "\n",
    "# C2\n",
    "neg_count_df21 = negative_rows.groupby(['item_author']).size().reset_index(name='neg_count21')\n",
    "neg_count_df22 = negative_rows.groupby(['article_author']).size().reset_index(name='neg_count22')\n",
    "neg_count_df23 = negative_rows.groupby(['article_source']).size().reset_index(name='neg_count23')\n",
    "\n",
    "merged = merged.merge(neg_count_df21, on='item_author', how='left', suffixes=('', '21')).fillna(0)\n",
    "merged = merged.merge(neg_count_df22, on='article_author', how='left', suffixes=('', '22')).fillna(0)\n",
    "merged = merged.merge(neg_count_df23, on='article_source', how='left', suffixes=('', '23')).fillna(0)\n",
    "merged['neg_count2'] = merged.apply(lambda row: row['neg_count21'] + row['neg_count22'] + row['neg_count23'], axis=1)\n",
    "\n",
    "# C3\n",
    "neg_count_df3 = negative_rows.groupby(['stock_code']).size().reset_index(name='neg_count3')\n",
    "# Merge with the previous result on stock_code\n",
    "merged = merged.merge(neg_count_df3, on='stock_code', how='left', suffixes=('', '3')).fillna(0)\n",
    "\n",
    "# Adjust neg_count2 based on whether neg_count was found\n",
    "# merged['neg_count2'] = merged.apply(lambda row: row['neg_count2'] if row['neg_count'] == 0 else 0, axis=1)\n",
    "merged['total'] = merged.apply(lambda row: row['neg_count1'] + row['neg_count2'] + row['neg_count3'], axis=1)\n",
    "\n",
    "# Convert to lists if needed\n",
    "neg_count1 = merged['neg_count1'].tolist()\n",
    "neg_count21 = merged['neg_count21'].tolist()\n",
    "neg_count22 = merged['neg_count22'].tolist()\n",
    "neg_count23 = merged['neg_count23'].tolist()\n",
    "neg_count2 = merged['neg_count2'].tolist()\n",
    "neg_count3 = merged['neg_count3'].tolist()\n",
    "neg_count_t = merged['total'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def analyse_neg_sample(neg_count1, neg_count2=None):\n",
    "    if neg_count2:\n",
    "        neg_count = np.array([sum(x) for x in zip(neg_count1, neg_count2)])\n",
    "    else:\n",
    "        neg_count = np.array(neg_count1)\n",
    "\n",
    "    # Mean of the array\n",
    "    mean = np.mean(neg_count)\n",
    "\n",
    "    # Median of the array\n",
    "    median = np.median(neg_count)\n",
    "\n",
    "    # Count of zeros in the array\n",
    "    zero_count = np.size(neg_count) - np.count_nonzero(neg_count)\n",
    "\n",
    "    print(f\"Mean: {mean}\")\n",
    "    print(f\"Median: {median}\")\n",
    "    print(f\"Zero count: {zero_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 609.5727251467198\n",
      "Median: 3.0\n",
      "Zero count: 63527\n"
     ]
    }
   ],
   "source": [
    "analyse_neg_sample(neg_count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 2391.9262633124345\n",
      "Median: 937.0\n",
      "Zero count: 535\n"
     ]
    }
   ],
   "source": [
    "analyse_neg_sample(neg_count1, neg_count21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 1906228.6984754503\n",
      "Median: 2221475.0\n",
      "Zero count: 2945\n"
     ]
    }
   ],
   "source": [
    "analyse_neg_sample(neg_count1, neg_count22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 181237.7250539179\n",
      "Median: 21698.0\n",
      "Zero count: 12\n"
     ]
    }
   ],
   "source": [
    "analyse_neg_sample(neg_count1, neg_count23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 2088029.6316172404\n",
      "Median: 2243655.0\n",
      "Zero count: 0\n"
     ]
    }
   ],
   "source": [
    "analyse_neg_sample(neg_count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 1259.1293361693597\n",
      "Median: 1320.0\n",
      "Zero count: 530\n"
     ]
    }
   ],
   "source": [
    "analyse_neg_sample(neg_count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 1868.7020613160796\n",
      "Median: 1348.0\n",
      "Zero count: 530\n"
     ]
    }
   ],
   "source": [
    "analyse_neg_sample(neg_count1, neg_count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 3041.4828743350745\n",
      "Median: 2248.0\n",
      "Zero count: 530\n"
     ]
    }
   ],
   "source": [
    "analyse_neg_sample(neg_count21, neg_count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 1906878.2550864727\n",
      "Median: 2222497.0\n",
      "Zero count: 2\n"
     ]
    }
   ],
   "source": [
    "analyse_neg_sample(neg_count22, neg_count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 181887.28166494056\n",
      "Median: 22521.0\n",
      "Zero count: 0\n"
     ]
    }
   ],
   "source": [
    "analyse_neg_sample(neg_count23, neg_count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MTL-4YFQ4-cY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
